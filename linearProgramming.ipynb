{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- target = 251\n",
    "- solution is 1 for all elements except 34 and 22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[25, 34]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [25,34]\n",
    "#         ,16,77,22,88,45]\n",
    "\n",
    "\n",
    "print(data) \n",
    "type(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function linprog in module scipy.optimize._linprog:\n",
      "\n",
      "linprog(c, A_ub=None, b_ub=None, A_eq=None, b_eq=None, bounds=None, method='simplex', callback=None, options=None)\n",
      "    Minimize a linear objective function subject to linear\n",
      "    equality and inequality constraints.\n",
      "    \n",
      "    Linear Programming is intended to solve the following problem form::\n",
      "    \n",
      "        Minimize:     c^T * x\n",
      "    \n",
      "        Subject to:   A_ub * x <= b_ub\n",
      "                      A_eq * x == b_eq\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    c : array_like\n",
      "        Coefficients of the linear objective function to be minimized.\n",
      "    A_ub : array_like, optional\n",
      "        2-D array which, when matrix-multiplied by ``x``, gives the values of\n",
      "        the upper-bound inequality constraints at ``x``.\n",
      "    b_ub : array_like, optional\n",
      "        1-D array of values representing the upper-bound of each inequality\n",
      "        constraint (row) in ``A_ub``.\n",
      "    A_eq : array_like, optional\n",
      "        2-D array which, when matrix-multiplied by ``x``, gives the values of\n",
      "        the equality constraints at ``x``.\n",
      "    b_eq : array_like, optional\n",
      "        1-D array of values representing the RHS of each equality constraint\n",
      "        (row) in ``A_eq``.\n",
      "    bounds : sequence, optional\n",
      "        ``(min, max)`` pairs for each element in ``x``, defining\n",
      "        the bounds on that parameter. Use None for one of ``min`` or\n",
      "        ``max`` when there is no bound in that direction. By default\n",
      "        bounds are ``(0, None)`` (non-negative)\n",
      "        If a sequence containing a single tuple is provided, then ``min`` and\n",
      "        ``max`` will be applied to all variables in the problem.\n",
      "    method : str, optional\n",
      "        Type of solver.  :ref:`'simplex' <optimize.linprog-simplex>`\n",
      "        and :ref:`'interior-point' <optimize.linprog-interior-point>`\n",
      "        are supported.\n",
      "    callback : callable, optional (simplex only)\n",
      "        If a callback function is provide, it will be called within each\n",
      "        iteration of the simplex algorithm. The callback must have the\n",
      "        signature ``callback(xk, **kwargs)`` where ``xk`` is the current\n",
      "        solution vector and ``kwargs`` is a dictionary containing the\n",
      "        following::\n",
      "    \n",
      "            \"tableau\" : The current Simplex algorithm tableau\n",
      "            \"nit\" : The current iteration.\n",
      "            \"pivot\" : The pivot (row, column) used for the next iteration.\n",
      "            \"phase\" : Whether the algorithm is in Phase 1 or Phase 2.\n",
      "            \"basis\" : The indices of the columns of the basic variables.\n",
      "    \n",
      "    options : dict, optional\n",
      "        A dictionary of solver options. All methods accept the following\n",
      "        generic options:\n",
      "    \n",
      "            maxiter : int\n",
      "                Maximum number of iterations to perform.\n",
      "            disp : bool\n",
      "                Set to True to print convergence messages.\n",
      "    \n",
      "        For method-specific options, see :func:`show_options('linprog')`.\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    A `scipy.optimize.OptimizeResult` consisting of the following fields:\n",
      "    \n",
      "        x : ndarray\n",
      "            The independent variable vector which optimizes the linear\n",
      "            programming problem.\n",
      "        fun : float\n",
      "            Value of the objective function.\n",
      "        slack : ndarray\n",
      "            The values of the slack variables.  Each slack variable corresponds\n",
      "            to an inequality constraint.  If the slack is zero, then the\n",
      "            corresponding constraint is active.\n",
      "        success : bool\n",
      "            Returns True if the algorithm succeeded in finding an optimal\n",
      "            solution.\n",
      "        status : int\n",
      "            An integer representing the exit status of the optimization::\n",
      "    \n",
      "                 0 : Optimization terminated successfully\n",
      "                 1 : Iteration limit reached\n",
      "                 2 : Problem appears to be infeasible\n",
      "                 3 : Problem appears to be unbounded\n",
      "    \n",
      "        nit : int\n",
      "            The number of iterations performed.\n",
      "        message : str\n",
      "            A string descriptor of the exit status of the optimization.\n",
      "    \n",
      "    See Also\n",
      "    --------\n",
      "    show_options : Additional options accepted by the solvers\n",
      "    \n",
      "    Notes\n",
      "    -----\n",
      "    This section describes the available solvers that can be selected by the\n",
      "    'method' parameter. The default method\n",
      "    is :ref:`Simplex <optimize.linprog-simplex>`.\n",
      "    :ref:`Interior point <optimize.linprog-interior-point>` is also available.\n",
      "    \n",
      "    Method *simplex* uses the simplex algorithm (as it relates to linear\n",
      "    programming, NOT the Nelder-Mead simplex) [1]_, [2]_. This algorithm\n",
      "    should be reasonably reliable and fast for small problems.\n",
      "    \n",
      "    .. versionadded:: 0.15.0\n",
      "    \n",
      "    Method *interior-point* uses the primal-dual path following algorithm\n",
      "    as outlined in [4]_. This algorithm is intended to provide a faster\n",
      "    and more reliable alternative to *simplex*, especially for large,\n",
      "    sparse problems. Note, however, that the solution returned may be slightly\n",
      "    less accurate than that of the simplex method and may not correspond with a\n",
      "    vertex of the polytope defined by the constraints.\n",
      "    \n",
      "    References\n",
      "    ----------\n",
      "    .. [1] Dantzig, George B., Linear programming and extensions. Rand\n",
      "           Corporation Research Study Princeton Univ. Press, Princeton, NJ,\n",
      "           1963\n",
      "    .. [2] Hillier, S.H. and Lieberman, G.J. (1995), \"Introduction to\n",
      "           Mathematical Programming\", McGraw-Hill, Chapter 4.\n",
      "    .. [3] Bland, Robert G. New finite pivoting rules for the simplex method.\n",
      "           Mathematics of Operations Research (2), 1977: pp. 103-107.\n",
      "    .. [4] Andersen, Erling D., and Knud D. Andersen. \"The MOSEK interior point\n",
      "           optimizer for linear programming: an implementation of the\n",
      "           homogeneous algorithm.\" High performance optimization. Springer US,\n",
      "           2000. 197-232.\n",
      "    .. [5] Andersen, Erling D. \"Finding all linearly dependent rows in\n",
      "           large-scale linear programming.\" Optimization Methods and Software\n",
      "           6.3 (1995): 219-227.\n",
      "    .. [6] Freund, Robert M. \"Primal-Dual Interior-Point Methods for Linear\n",
      "           Programming based on Newton's Method.\" Unpublished Course Notes,\n",
      "           March 2004. Available 2/25/2017 at\n",
      "           https://ocw.mit.edu/courses/sloan-school-of-management/15-084j-nonlinear-programming-spring-2004/lecture-notes/lec14_int_pt_mthd.pdf\n",
      "    .. [7] Fourer, Robert. \"Solving Linear Programs by Interior-Point Methods.\"\n",
      "           Unpublished Course Notes, August 26, 2005. Available 2/25/2017 at\n",
      "           http://www.4er.org/CourseNotes/Book%20B/B-III.pdf\n",
      "    .. [8] Andersen, Erling D., and Knud D. Andersen. \"Presolving in linear\n",
      "           programming.\" Mathematical Programming 71.2 (1995): 221-245.\n",
      "    .. [9] Bertsimas, Dimitris, and J. Tsitsiklis. \"Introduction to linear\n",
      "           programming.\" Athena Scientific 1 (1997): 997.\n",
      "    .. [10] Andersen, Erling D., et al. Implementation of interior point\n",
      "            methods for large scale linear programming. HEC/Universite de\n",
      "            Geneve, 1996.\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    Consider the following problem:\n",
      "    \n",
      "    Minimize: f = -1*x[0] + 4*x[1]\n",
      "    \n",
      "    Subject to: -3*x[0] + 1*x[1] <= 6\n",
      "                 1*x[0] + 2*x[1] <= 4\n",
      "                            x[1] >= -3\n",
      "    \n",
      "    where:  -inf <= x[0] <= inf\n",
      "    \n",
      "    This problem deviates from the standard linear programming problem.\n",
      "    In standard form, linear programming problems assume the variables x are\n",
      "    non-negative.  Since the variables don't have standard bounds where\n",
      "    0 <= x <= inf, the bounds of the variables must be explicitly set.\n",
      "    \n",
      "    There are two upper-bound constraints, which can be expressed as\n",
      "    \n",
      "    dot(A_ub, x) <= b_ub\n",
      "    \n",
      "    The input for this problem is as follows:\n",
      "    \n",
      "    >>> c = [-1, 4]\n",
      "    >>> A = [[-3, 1], [1, 2]]\n",
      "    >>> b = [6, 4]\n",
      "    >>> x0_bounds = (None, None)\n",
      "    >>> x1_bounds = (-3, None)\n",
      "    >>> from scipy.optimize import linprog\n",
      "    >>> res = linprog(c, A_ub=A, b_ub=b, bounds=(x0_bounds, x1_bounds),\n",
      "    ...               options={\"disp\": True})\n",
      "    Optimization terminated successfully.\n",
      "         Current function value: -22.000000\n",
      "         Iterations: 1\n",
      "    >>> print(res)\n",
      "         fun: -22.0\n",
      "     message: 'Optimization terminated successfully.'\n",
      "         nit: 1\n",
      "       slack: array([39.,  0.])\n",
      "      status: 0\n",
      "     success: True\n",
      "           x: array([10., -3.])\n",
      "    \n",
      "    Note the actual objective value is 11.428571.  In this case we minimized\n",
      "    the negative of the objective function.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from scipy.optimize import linprog\n",
    "help(linprog)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tuple"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x0_bounds = (None, None)\n",
    "type(x0_bounds)\n",
    "# x1_bounds = (251, None)\n",
    "# res = linprog(data,bounds=(x0_bounds, x1_bounds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     fun: -0.0\n",
      " message: 'Optimization terminated successfully.'\n",
      "     nit: 0\n",
      "   slack: array([], dtype=float64)\n",
      "  status: 0\n",
      " success: True\n",
      "       x: array([0., 0., 0., 0., 0., 0., 0.])\n"
     ]
    }
   ],
   "source": [
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function minimize in module scipy.optimize._minimize:\n",
      "\n",
      "minimize(fun, x0, args=(), method=None, jac=None, hess=None, hessp=None, bounds=None, constraints=(), tol=None, callback=None, options=None)\n",
      "    Minimization of scalar function of one or more variables.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    fun : callable\n",
      "        The objective function to be minimized.\n",
      "    \n",
      "            ``fun(x, *args) -> float``\n",
      "    \n",
      "        where x is an 1-D array with shape (n,) and `args`\n",
      "        is a tuple of the fixed parameters needed to completely\n",
      "        specify the function.\n",
      "    x0 : ndarray, shape (n,)\n",
      "        Initial guess. Array of real elements of size (n,),\n",
      "        where 'n' is the number of independent variables.\n",
      "    args : tuple, optional\n",
      "        Extra arguments passed to the objective function and its\n",
      "        derivatives (`fun`, `jac` and `hess` functions).\n",
      "    method : str or callable, optional\n",
      "        Type of solver.  Should be one of\n",
      "    \n",
      "            - 'Nelder-Mead' :ref:`(see here) <optimize.minimize-neldermead>`\n",
      "            - 'Powell'      :ref:`(see here) <optimize.minimize-powell>`\n",
      "            - 'CG'          :ref:`(see here) <optimize.minimize-cg>`\n",
      "            - 'BFGS'        :ref:`(see here) <optimize.minimize-bfgs>`\n",
      "            - 'Newton-CG'   :ref:`(see here) <optimize.minimize-newtoncg>`\n",
      "            - 'L-BFGS-B'    :ref:`(see here) <optimize.minimize-lbfgsb>`\n",
      "            - 'TNC'         :ref:`(see here) <optimize.minimize-tnc>`\n",
      "            - 'COBYLA'      :ref:`(see here) <optimize.minimize-cobyla>`\n",
      "            - 'SLSQP'       :ref:`(see here) <optimize.minimize-slsqp>`\n",
      "            - 'trust-constr':ref:`(see here) <optimize.minimize-trustconstr>`\n",
      "            - 'dogleg'      :ref:`(see here) <optimize.minimize-dogleg>`\n",
      "            - 'trust-ncg'   :ref:`(see here) <optimize.minimize-trustncg>`\n",
      "            - 'trust-exact' :ref:`(see here) <optimize.minimize-trustexact>`\n",
      "            - 'trust-krylov' :ref:`(see here) <optimize.minimize-trustkrylov>`\n",
      "            - custom - a callable object (added in version 0.14.0),\n",
      "              see below for description.\n",
      "    \n",
      "        If not given, chosen to be one of ``BFGS``, ``L-BFGS-B``, ``SLSQP``,\n",
      "        depending if the problem has constraints or bounds.\n",
      "    jac : {callable,  '2-point', '3-point', 'cs', bool}, optional\n",
      "        Method for computing the gradient vector. Only for CG, BFGS,\n",
      "        Newton-CG, L-BFGS-B, TNC, SLSQP, dogleg, trust-ncg, trust-krylov,\n",
      "        trust-exact and trust-constr. If it is a callable, it should be a\n",
      "        function that returns the gradient vector:\n",
      "    \n",
      "            ``jac(x, *args) -> array_like, shape (n,)``\n",
      "    \n",
      "        where x is an array with shape (n,) and `args` is a tuple with\n",
      "        the fixed parameters. Alternatively, the keywords\n",
      "        {'2-point', '3-point', 'cs'} select a finite\n",
      "        difference scheme for numerical estimation of the gradient. Options\n",
      "        '3-point' and 'cs' are available only to 'trust-constr'.\n",
      "        If `jac` is a Boolean and is True, `fun` is assumed to return the\n",
      "        gradient along with the objective function. If False, the gradient\n",
      "        will be estimated using '2-point' finite difference estimation.\n",
      "    hess : {callable, '2-point', '3-point', 'cs', HessianUpdateStrategy},  optional\n",
      "        Method for computing the Hessian matrix. Only for Newton-CG, dogleg,\n",
      "        trust-ncg,  trust-krylov, trust-exact and trust-constr. If it is\n",
      "        callable, it should return the  Hessian matrix:\n",
      "    \n",
      "            ``hess(x, *args) -> {LinearOperator, spmatrix, array}, (n, n)``\n",
      "    \n",
      "        where x is a (n,) ndarray and `args` is a tuple with the fixed\n",
      "        parameters. LinearOperator and sparse matrix returns are\n",
      "        allowed only for 'trust-constr' method. Alternatively, the keywords\n",
      "        {'2-point', '3-point', 'cs'} select a finite difference scheme\n",
      "        for numerical estimation. Or, objects implementing\n",
      "        `HessianUpdateStrategy` interface can be used to approximate\n",
      "        the Hessian. Available quasi-Newton methods implementing\n",
      "        this interface are:\n",
      "    \n",
      "            - `BFGS`;\n",
      "            - `SR1`.\n",
      "    \n",
      "        Whenever the gradient is estimated via finite-differences,\n",
      "        the Hessian cannot be estimated with options\n",
      "        {'2-point', '3-point', 'cs'} and needs to be\n",
      "        estimated using one of the quasi-Newton strategies.\n",
      "        Finite-difference options {'2-point', '3-point', 'cs'} and\n",
      "        `HessianUpdateStrategy` are available only for 'trust-constr' method.\n",
      "    hessp : callable, optional\n",
      "        Hessian of objective function times an arbitrary vector p. Only for\n",
      "        Newton-CG, trust-ncg, trust-krylov, trust-constr.\n",
      "        Only one of `hessp` or `hess` needs to be given.  If `hess` is\n",
      "        provided, then `hessp` will be ignored.  `hessp` must compute the\n",
      "        Hessian times an arbitrary vector:\n",
      "    \n",
      "            ``hessp(x, p, *args) ->  ndarray shape (n,)``\n",
      "    \n",
      "        where x is a (n,) ndarray, p is an arbitrary vector with\n",
      "        dimension (n,) and `args` is a tuple with the fixed\n",
      "        parameters.\n",
      "    bounds : sequence or `Bounds`, optional\n",
      "        Bounds on variables for L-BFGS-B, TNC, SLSQP and\n",
      "        trust-constr methods. There are two ways to specify the bounds:\n",
      "    \n",
      "            1. Instance of `Bounds` class.\n",
      "            2. Sequence of ``(min, max)`` pairs for each element in `x`. None\n",
      "               is used to specify no bound.\n",
      "    \n",
      "    constraints : {Constraint, dict} or List of {Constraint, dict}, optional\n",
      "        Constraints definition (only for COBYLA, SLSQP and trust-constr).\n",
      "        Constraints for 'trust-constr' are defined as a single object or a\n",
      "        list of objects specifying constraints to the optimization problem.\n",
      "        Available constraints are:\n",
      "    \n",
      "            - `LinearConstraint`\n",
      "            - `NonlinearConstraint`\n",
      "    \n",
      "        Constraints for COBYLA, SLSQP are defined as a list of dictionaries.\n",
      "        Each dictionary with fields:\n",
      "    \n",
      "            type : str\n",
      "                Constraint type: 'eq' for equality, 'ineq' for inequality.\n",
      "            fun : callable\n",
      "                The function defining the constraint.\n",
      "            jac : callable, optional\n",
      "                The Jacobian of `fun` (only for SLSQP).\n",
      "            args : sequence, optional\n",
      "                Extra arguments to be passed to the function and Jacobian.\n",
      "    \n",
      "        Equality constraint means that the constraint function result is to\n",
      "        be zero whereas inequality means that it is to be non-negative.\n",
      "        Note that COBYLA only supports inequality constraints.\n",
      "    tol : float, optional\n",
      "        Tolerance for termination. For detailed control, use solver-specific\n",
      "        options.\n",
      "    options : dict, optional\n",
      "        A dictionary of solver options. All methods accept the following\n",
      "        generic options:\n",
      "    \n",
      "            maxiter : int\n",
      "                Maximum number of iterations to perform.\n",
      "            disp : bool\n",
      "                Set to True to print convergence messages.\n",
      "    \n",
      "        For method-specific options, see :func:`show_options()`.\n",
      "    callback : callable, optional\n",
      "        Called after each iteration. For 'trust-constr' it is a callable with\n",
      "        the signature:\n",
      "    \n",
      "            ``callback(xk, OptimizeResult state) -> bool``\n",
      "    \n",
      "        where ``xk`` is the current parameter vector. and ``state``\n",
      "        is an `OptimizeResult` object, with the same fields\n",
      "        as the ones from the return.  If callback returns True\n",
      "        the algorithm execution is terminated.\n",
      "        For all the other methods, the signature is:\n",
      "    \n",
      "            ``callback(xk)``\n",
      "    \n",
      "        where ``xk`` is the current parameter vector.\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    res : OptimizeResult\n",
      "        The optimization result represented as a ``OptimizeResult`` object.\n",
      "        Important attributes are: ``x`` the solution array, ``success`` a\n",
      "        Boolean flag indicating if the optimizer exited successfully and\n",
      "        ``message`` which describes the cause of the termination. See\n",
      "        `OptimizeResult` for a description of other attributes.\n",
      "    \n",
      "    \n",
      "    See also\n",
      "    --------\n",
      "    minimize_scalar : Interface to minimization algorithms for scalar\n",
      "        univariate functions\n",
      "    show_options : Additional options accepted by the solvers\n",
      "    \n",
      "    Notes\n",
      "    -----\n",
      "    This section describes the available solvers that can be selected by the\n",
      "    'method' parameter. The default method is *BFGS*.\n",
      "    \n",
      "    **Unconstrained minimization**\n",
      "    \n",
      "    Method :ref:`Nelder-Mead <optimize.minimize-neldermead>` uses the\n",
      "    Simplex algorithm [1]_, [2]_. This algorithm is robust in many\n",
      "    applications. However, if numerical computation of derivative can be\n",
      "    trusted, other algorithms using the first and/or second derivatives\n",
      "    information might be preferred for their better performance in\n",
      "    general.\n",
      "    \n",
      "    Method :ref:`Powell <optimize.minimize-powell>` is a modification\n",
      "    of Powell's method [3]_, [4]_ which is a conjugate direction\n",
      "    method. It performs sequential one-dimensional minimizations along\n",
      "    each vector of the directions set (`direc` field in `options` and\n",
      "    `info`), which is updated at each iteration of the main\n",
      "    minimization loop. The function need not be differentiable, and no\n",
      "    derivatives are taken.\n",
      "    \n",
      "    Method :ref:`CG <optimize.minimize-cg>` uses a nonlinear conjugate\n",
      "    gradient algorithm by Polak and Ribiere, a variant of the\n",
      "    Fletcher-Reeves method described in [5]_ pp.  120-122. Only the\n",
      "    first derivatives are used.\n",
      "    \n",
      "    Method :ref:`BFGS <optimize.minimize-bfgs>` uses the quasi-Newton\n",
      "    method of Broyden, Fletcher, Goldfarb, and Shanno (BFGS) [5]_\n",
      "    pp. 136. It uses the first derivatives only. BFGS has proven good\n",
      "    performance even for non-smooth optimizations. This method also\n",
      "    returns an approximation of the Hessian inverse, stored as\n",
      "    `hess_inv` in the OptimizeResult object.\n",
      "    \n",
      "    Method :ref:`Newton-CG <optimize.minimize-newtoncg>` uses a\n",
      "    Newton-CG algorithm [5]_ pp. 168 (also known as the truncated\n",
      "    Newton method). It uses a CG method to the compute the search\n",
      "    direction. See also *TNC* method for a box-constrained\n",
      "    minimization with a similar algorithm. Suitable for large-scale\n",
      "    problems.\n",
      "    \n",
      "    Method :ref:`dogleg <optimize.minimize-dogleg>` uses the dog-leg\n",
      "    trust-region algorithm [5]_ for unconstrained minimization. This\n",
      "    algorithm requires the gradient and Hessian; furthermore the\n",
      "    Hessian is required to be positive definite.\n",
      "    \n",
      "    Method :ref:`trust-ncg <optimize.minimize-trustncg>` uses the\n",
      "    Newton conjugate gradient trust-region algorithm [5]_ for\n",
      "    unconstrained minimization. This algorithm requires the gradient\n",
      "    and either the Hessian or a function that computes the product of\n",
      "    the Hessian with a given vector. Suitable for large-scale problems.\n",
      "    \n",
      "    Method :ref:`trust-krylov <optimize.minimize-trustkrylov>` uses\n",
      "    the Newton GLTR trust-region algorithm [14]_, [15]_ for unconstrained\n",
      "    minimization. This algorithm requires the gradient\n",
      "    and either the Hessian or a function that computes the product of\n",
      "    the Hessian with a given vector. Suitable for large-scale problems.\n",
      "    On indefinite problems it requires usually less iterations than the\n",
      "    `trust-ncg` method and is recommended for medium and large-scale problems.\n",
      "    \n",
      "    Method :ref:`trust-exact <optimize.minimize-trustexact>`\n",
      "    is a trust-region method for unconstrained minimization in which\n",
      "    quadratic subproblems are solved almost exactly [13]_. This\n",
      "    algorithm requires the gradient and the Hessian (which is\n",
      "    *not* required to be positive definite). It is, in many\n",
      "    situations, the Newton method to converge in fewer iteraction\n",
      "    and the most recommended for small and medium-size problems.\n",
      "    \n",
      "    **Bound-Constrained minimization**\n",
      "    \n",
      "    Method :ref:`L-BFGS-B <optimize.minimize-lbfgsb>` uses the L-BFGS-B\n",
      "    algorithm [6]_, [7]_ for bound constrained minimization.\n",
      "    \n",
      "    Method :ref:`TNC <optimize.minimize-tnc>` uses a truncated Newton\n",
      "    algorithm [5]_, [8]_ to minimize a function with variables subject\n",
      "    to bounds. This algorithm uses gradient information; it is also\n",
      "    called Newton Conjugate-Gradient. It differs from the *Newton-CG*\n",
      "    method described above as it wraps a C implementation and allows\n",
      "    each variable to be given upper and lower bounds.\n",
      "    \n",
      "    **Constrained Minimization**\n",
      "    \n",
      "    Method :ref:`COBYLA <optimize.minimize-cobyla>` uses the\n",
      "    Constrained Optimization BY Linear Approximation (COBYLA) method\n",
      "    [9]_, [10]_, [11]_. The algorithm is based on linear\n",
      "    approximations to the objective function and each constraint. The\n",
      "    method wraps a FORTRAN implementation of the algorithm. The\n",
      "    constraints functions 'fun' may return either a single number\n",
      "    or an array or list of numbers.\n",
      "    \n",
      "    Method :ref:`SLSQP <optimize.minimize-slsqp>` uses Sequential\n",
      "    Least SQuares Programming to minimize a function of several\n",
      "    variables with any combination of bounds, equality and inequality\n",
      "    constraints. The method wraps the SLSQP Optimization subroutine\n",
      "    originally implemented by Dieter Kraft [12]_. Note that the\n",
      "    wrapper handles infinite values in bounds by converting them into\n",
      "    large floating values.\n",
      "    \n",
      "    Method :ref:`trust-constr <optimize.minimize-trustconstr>` is a\n",
      "    trust-region algorithm for constrained optimization. It swiches\n",
      "    between two implementations depending on the problem definition.\n",
      "    It is the most versatile constrained minimization algorithm\n",
      "    implemented in SciPy and the most appropriate for large-scale problems.\n",
      "    For equality constrained problems it is an implementation of Byrd-Omojokun\n",
      "    Trust-Region SQP method described in [17]_ and in [5]_, p. 549. When\n",
      "    inequality constraints  are imposed as well, it swiches to the trust-region\n",
      "    interior point  method described in [16]_. This interior point algorithm,\n",
      "    in turn, solves inequality constraints by introducing slack variables\n",
      "    and solving a sequence of equality-constrained barrier problems\n",
      "    for progressively smaller values of the barrier parameter.\n",
      "    The previously described equality constrained SQP method is\n",
      "    used to solve the subproblems with increasing levels of accuracy\n",
      "    as the iterate gets closer to a solution.\n",
      "    \n",
      "    **Finite-Difference Options**\n",
      "    \n",
      "    For Method :ref:`trust-constr <optimize.minimize-trustconstr>`\n",
      "    the gradient and the Hessian may be approximated using\n",
      "    three finite-difference schemes: {'2-point', '3-point', 'cs'}.\n",
      "    The scheme 'cs' is, potentially, the most accurate but it\n",
      "    requires the function to correctly handles complex inputs and to\n",
      "    be differentiable in the complex plane. The scheme '3-point' is more\n",
      "    accurate than '2-point' but requires twice as much operations.\n",
      "    \n",
      "    **Custom minimizers**\n",
      "    \n",
      "    It may be useful to pass a custom minimization method, for example\n",
      "    when using a frontend to this method such as `scipy.optimize.basinhopping`\n",
      "    or a different library.  You can simply pass a callable as the ``method``\n",
      "    parameter.\n",
      "    \n",
      "    The callable is called as ``method(fun, x0, args, **kwargs, **options)``\n",
      "    where ``kwargs`` corresponds to any other parameters passed to `minimize`\n",
      "    (such as `callback`, `hess`, etc.), except the `options` dict, which has\n",
      "    its contents also passed as `method` parameters pair by pair.  Also, if\n",
      "    `jac` has been passed as a bool type, `jac` and `fun` are mangled so that\n",
      "    `fun` returns just the function values and `jac` is converted to a function\n",
      "    returning the Jacobian.  The method shall return an ``OptimizeResult``\n",
      "    object.\n",
      "    \n",
      "    The provided `method` callable must be able to accept (and possibly ignore)\n",
      "    arbitrary parameters; the set of parameters accepted by `minimize` may\n",
      "    expand in future versions and then these parameters will be passed to\n",
      "    the method.  You can find an example in the scipy.optimize tutorial.\n",
      "    \n",
      "    .. versionadded:: 0.11.0\n",
      "    \n",
      "    References\n",
      "    ----------\n",
      "    .. [1] Nelder, J A, and R Mead. 1965. A Simplex Method for Function\n",
      "        Minimization. The Computer Journal 7: 308-13.\n",
      "    .. [2] Wright M H. 1996. Direct search methods: Once scorned, now\n",
      "        respectable, in Numerical Analysis 1995: Proceedings of the 1995\n",
      "        Dundee Biennial Conference in Numerical Analysis (Eds. D F\n",
      "        Griffiths and G A Watson). Addison Wesley Longman, Harlow, UK.\n",
      "        191-208.\n",
      "    .. [3] Powell, M J D. 1964. An efficient method for finding the minimum of\n",
      "       a function of several variables without calculating derivatives. The\n",
      "       Computer Journal 7: 155-162.\n",
      "    .. [4] Press W, S A Teukolsky, W T Vetterling and B P Flannery.\n",
      "       Numerical Recipes (any edition), Cambridge University Press.\n",
      "    .. [5] Nocedal, J, and S J Wright. 2006. Numerical Optimization.\n",
      "       Springer New York.\n",
      "    .. [6] Byrd, R H and P Lu and J. Nocedal. 1995. A Limited Memory\n",
      "       Algorithm for Bound Constrained Optimization. SIAM Journal on\n",
      "       Scientific and Statistical Computing 16 (5): 1190-1208.\n",
      "    .. [7] Zhu, C and R H Byrd and J Nocedal. 1997. L-BFGS-B: Algorithm\n",
      "       778: L-BFGS-B, FORTRAN routines for large scale bound constrained\n",
      "       optimization. ACM Transactions on Mathematical Software 23 (4):\n",
      "       550-560.\n",
      "    .. [8] Nash, S G. Newton-Type Minimization Via the Lanczos Method.\n",
      "       1984. SIAM Journal of Numerical Analysis 21: 770-778.\n",
      "    .. [9] Powell, M J D. A direct search optimization method that models\n",
      "       the objective and constraint functions by linear interpolation.\n",
      "       1994. Advances in Optimization and Numerical Analysis, eds. S. Gomez\n",
      "       and J-P Hennart, Kluwer Academic (Dordrecht), 51-67.\n",
      "    .. [10] Powell M J D. Direct search algorithms for optimization\n",
      "       calculations. 1998. Acta Numerica 7: 287-336.\n",
      "    .. [11] Powell M J D. A view of algorithms for optimization without\n",
      "       derivatives. 2007.Cambridge University Technical Report DAMTP\n",
      "       2007/NA03\n",
      "    .. [12] Kraft, D. A software package for sequential quadratic\n",
      "       programming. 1988. Tech. Rep. DFVLR-FB 88-28, DLR German Aerospace\n",
      "       Center -- Institute for Flight Mechanics, Koln, Germany.\n",
      "    .. [13] Conn, A. R., Gould, N. I., and Toint, P. L.\n",
      "       Trust region methods. 2000. Siam. pp. 169-200.\n",
      "    .. [14] F. Lenders, C. Kirches, A. Potschka: \"trlib: A vector-free\n",
      "       implementation of the GLTR method for iterative solution of\n",
      "       the trust region problem\", https://arxiv.org/abs/1611.04718\n",
      "    .. [15] N. Gould, S. Lucidi, M. Roma, P. Toint: \"Solving the\n",
      "       Trust-Region Subproblem using the Lanczos Method\",\n",
      "       SIAM J. Optim., 9(2), 504--525, (1999).\n",
      "    .. [16] Byrd, Richard H., Mary E. Hribar, and Jorge Nocedal. 1999.\n",
      "        An interior point algorithm for large-scale nonlinear  programming.\n",
      "        SIAM Journal on Optimization 9.4: 877-900.\n",
      "    .. [17] Lalee, Marucha, Jorge Nocedal, and Todd Plantega. 1998. On the\n",
      "        implementation of an algorithm for large-scale equality constrained\n",
      "        optimization. SIAM Journal on Optimization 8.3: 682-706.\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    Let us consider the problem of minimizing the Rosenbrock function. This\n",
      "    function (and its respective derivatives) is implemented in `rosen`\n",
      "    (resp. `rosen_der`, `rosen_hess`) in the `scipy.optimize`.\n",
      "    \n",
      "    >>> from scipy.optimize import minimize, rosen, rosen_der\n",
      "    \n",
      "    A simple application of the *Nelder-Mead* method is:\n",
      "    \n",
      "    >>> x0 = [1.3, 0.7, 0.8, 1.9, 1.2]\n",
      "    >>> res = minimize(rosen, x0, method='Nelder-Mead', tol=1e-6)\n",
      "    >>> res.x\n",
      "    array([ 1.,  1.,  1.,  1.,  1.])\n",
      "    \n",
      "    Now using the *BFGS* algorithm, using the first derivative and a few\n",
      "    options:\n",
      "    \n",
      "    >>> res = minimize(rosen, x0, method='BFGS', jac=rosen_der,\n",
      "    ...                options={'gtol': 1e-6, 'disp': True})\n",
      "    Optimization terminated successfully.\n",
      "             Current function value: 0.000000\n",
      "             Iterations: 26\n",
      "             Function evaluations: 31\n",
      "             Gradient evaluations: 31\n",
      "    >>> res.x\n",
      "    array([ 1.,  1.,  1.,  1.,  1.])\n",
      "    >>> print(res.message)\n",
      "    Optimization terminated successfully.\n",
      "    >>> res.hess_inv\n",
      "    array([[ 0.00749589,  0.01255155,  0.02396251,  0.04750988,  0.09495377],  # may vary\n",
      "           [ 0.01255155,  0.02510441,  0.04794055,  0.09502834,  0.18996269],\n",
      "           [ 0.02396251,  0.04794055,  0.09631614,  0.19092151,  0.38165151],\n",
      "           [ 0.04750988,  0.09502834,  0.19092151,  0.38341252,  0.7664427 ],\n",
      "           [ 0.09495377,  0.18996269,  0.38165151,  0.7664427,   1.53713523]])\n",
      "    \n",
      "    \n",
      "    Next, consider a minimization problem with several constraints (namely\n",
      "    Example 16.4 from [5]_). The objective function is:\n",
      "    \n",
      "    >>> fun = lambda x: (x[0] - 1)**2 + (x[1] - 2.5)**2\n",
      "    \n",
      "    There are three constraints defined as:\n",
      "    \n",
      "    >>> cons = ({'type': 'ineq', 'fun': lambda x:  x[0] - 2 * x[1] + 2},\n",
      "    ...         {'type': 'ineq', 'fun': lambda x: -x[0] - 2 * x[1] + 6},\n",
      "    ...         {'type': 'ineq', 'fun': lambda x: -x[0] + 2 * x[1] + 2})\n",
      "    \n",
      "    And variables must be positive, hence the following bounds:\n",
      "    \n",
      "    >>> bnds = ((0, None), (0, None))\n",
      "    \n",
      "    The optimization problem is solved using the SLSQP method as:\n",
      "    \n",
      "    >>> res = minimize(fun, (2, 0), method='SLSQP', bounds=bnds,\n",
      "    ...                constraints=cons)\n",
      "    \n",
      "    It should converge to the theoretical solution (1.4 ,1.7).\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import minimize\n",
    "help(minimize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Maximum number of function evaluations has been exceeded.\n",
      "[-1.33656354e+51  1.19411258e+51  1.31469300e+51  7.97380685e+50\n",
      "  2.06994596e+51 -2.65346888e+52 -2.20617374e+51]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nathan/anaconda3/lib/python3.6/site-packages/scipy/optimize/optimize.py:478: DeprecationWarning: xtol is deprecated for Nelder-Mead, use xatol instead. If you specified both, only xatol is used.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "def fn(x):\n",
    "    return(sum(x)-251)\n",
    "x0 = np.array([25,34])\n",
    "x0 = np.array([25,34,16,77,22,88,45])\n",
    "# fn(x0)\n",
    "res = minimize(fn,x0,method='nelder-mead', options={'xtol': 1e-8, 'disp': True})\n",
    "print(res.x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cvxpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selection = cvxpy.Bool(len(x0))\n",
    "constraint = x0 * selection = 251\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class Problem in module cvxpy.problems.problem:\n",
      "\n",
      "class Problem(cvxpy.utilities.canonical.Canonical)\n",
      " |  A convex optimization problem.\n",
      " |  \n",
      " |  Problems are immutable, save for modification through the specification\n",
      " |  of :class:`~cvxpy.expressions.constants.parameters.Parameter`\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  objective : Minimize or Maximize\n",
      " |      The problem's objective.\n",
      " |  constraints : list\n",
      " |      The constraints on the problem variables.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      Problem\n",
      " |      cvxpy.utilities.canonical.Canonical\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __add__(self, other)\n",
      " |  \n",
      " |  __div__(self, other)\n",
      " |  \n",
      " |  __init__(self, objective, constraints=None)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  __mul__(self, other)\n",
      " |  \n",
      " |  __neg__(self)\n",
      " |  \n",
      " |  __radd__(self, other)\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __rmul__ = __mul__(self, other)\n",
      " |  \n",
      " |  __rsub__(self, other)\n",
      " |  \n",
      " |  __str__(self)\n",
      " |      Return str(self).\n",
      " |  \n",
      " |  __sub__(self, other)\n",
      " |  \n",
      " |  __truediv__ = __div__(self, other)\n",
      " |  \n",
      " |  atoms(self)\n",
      " |      Accessor method for atoms.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      list of :class:`~cvxpy.atoms.Atom`\n",
      " |          A list of the atom types in the problem; note that this list\n",
      " |          contains classes, not instances.\n",
      " |  \n",
      " |  constants(self)\n",
      " |      Accessor method for parameters.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      list of :class:`~cvxpy.expressions.constants.constant.Constant`\n",
      " |          A list of the constants in the problem.\n",
      " |  \n",
      " |  get_problem_data(self, solver)\n",
      " |      Returns the problem data used in the call to the solver.\n",
      " |      \n",
      " |      When a problem is solved, a chain of reductions, called a\n",
      " |      :class:`~cvxpy.reductions.solvers.solving_chain.SolvingChain`,\n",
      " |      compiles it to some low-level representation that is compatible with the\n",
      " |      targeted solver. This method returns that low-level representation.\n",
      " |      \n",
      " |      For some solving chains, this low-level representation is a dictionary\n",
      " |      that contains exactly those arguments that were supplied to the solver;\n",
      " |      however, for other solving chains, the data is an intermediate\n",
      " |      representation that is compiled even further by libraries other than\n",
      " |      CVXPY.\n",
      " |      \n",
      " |      A solution to the equivalent low-level problem can be obtained via the\n",
      " |      data by invoking the solve_via_data method of the returned solving\n",
      " |      chain, a thin wrapper around the code external to CVXPY that further\n",
      " |      processes and solves the problem. Invoke the unpack_results method\n",
      " |      to recover a solution to the original problem.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      solver : str\n",
      " |          The solver the problem data is for.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      dict or object\n",
      " |          lowest level representation of problem\n",
      " |      SolvingChain\n",
      " |          The solving chain that created the data.\n",
      " |      list\n",
      " |          The inverse data generated by the chain.\n",
      " |  \n",
      " |  is_dcp(self)\n",
      " |      Does the problem satisfy DCP rules?\n",
      " |  \n",
      " |  is_mixed_integer(self)\n",
      " |  \n",
      " |  is_qp(self)\n",
      " |      Is problem a quadratic program?\n",
      " |  \n",
      " |  parameters(self)\n",
      " |      Accessor method for parameters.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      list of :class:`~cvxpy.expressions.constants.parameter.Parameter`\n",
      " |          A list of the parameters in the problem.\n",
      " |  \n",
      " |  solve(self, *args, **kwargs)\n",
      " |      Solves the problem using the specified method.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      method : function\n",
      " |          The solve method to use.\n",
      " |      solver : str, optional\n",
      " |          The solver to use.\n",
      " |      verbose : bool, optional\n",
      " |          Overrides the default of hiding solver output.\n",
      " |      solver_specific_opts : dict, optional\n",
      " |          A dict of options that will be passed to the specific solver.\n",
      " |          In general, these options will override any default settings\n",
      " |          imposed by cvxpy.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      float\n",
      " |          The optimal value for the problem, or a string indicating\n",
      " |          why the problem could not be solved.\n",
      " |      \n",
      " |      Raises\n",
      " |      ------\n",
      " |      DCPError\n",
      " |          Raised if the problem is not DCP.\n",
      " |      SolverError\n",
      " |          Raised if no suitable solver exists among the installed solvers,\n",
      " |          or if an unanticipated error is encountered.\n",
      " |  \n",
      " |  unpack_results(self, solution, chain, inverse_data)\n",
      " |      Updates the problem state given the solver results.\n",
      " |      \n",
      " |      Updates problem.status, problem.value and value of\n",
      " |      primal and dual variables.\n",
      " |      \n",
      " |      Parameters\n",
      " |      __________\n",
      " |      solution : Solution\n",
      " |          The solution returned by applying the chain to the problem\n",
      " |          and invoking the solver on the resulting data.\n",
      " |      chain : SolvingChain\n",
      " |          A solving chain that was used to solve the problem.\n",
      " |      inverse_data : list\n",
      " |          The inverse data returned by applying the chain to the problem.\n",
      " |  \n",
      " |  variables(self)\n",
      " |      Accessor method for variables.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      list of :class:`~cvxpy.expressions.variable.Variable`\n",
      " |          A list of the variables in the problem.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods defined here:\n",
      " |  \n",
      " |  register_solve(name, func) from abc.ABCMeta\n",
      " |      Adds a solve method to the Problem class.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      name : str\n",
      " |          The keyword for the method.\n",
      " |      func : function\n",
      " |          The function that executes the solve method. This function must\n",
      " |          take as its first argument the problem instance to solve.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  constraints\n",
      " |      A shallow copy of the problem's constraints.\n",
      " |      \n",
      " |      Note that constraints cannot be reassigned, appended to, or otherwise\n",
      " |      modified after creation, except through parameters.\n",
      " |  \n",
      " |  objective\n",
      " |      Minimize or Maximize : The problem's objective.\n",
      " |      \n",
      " |      Note that the objective cannot be reassigned after creation,\n",
      " |      and modifying the objective after creation will result in\n",
      " |      undefined behavior.\n",
      " |  \n",
      " |  size_metrics\n",
      " |      :class:`~cvxpy.problems.problem.SizeMetrics` : Information about the problem's size.\n",
      " |  \n",
      " |  solver_stats\n",
      " |      :class:`~cvxpy.problems.problem.SolverStats` : Information returned by the solver.\n",
      " |  \n",
      " |  status\n",
      " |      str : The status from the last time the problem was solved; one\n",
      " |      of optimal, infeasible, or unbounded.\n",
      " |  \n",
      " |  value\n",
      " |      float : The value from the last time the problem was solved\n",
      " |      (or None if not solved).\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  REGISTERED_SOLVE_METHODS = {}\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from cvxpy.utilities.canonical.Canonical:\n",
      " |  \n",
      " |  copy(self, args=None, id_objects={})\n",
      " |      Returns a shallow copy of the object.\n",
      " |      \n",
      " |      Used to reconstruct an object tree.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      args : list, optional\n",
      " |          The arguments to reconstruct the object. If args=None, use the\n",
      " |          current args of the object.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      Expression\n",
      " |  \n",
      " |  get_data(self)\n",
      " |      Returns info needed to reconstruct the object besides the args.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      list\n",
      " |  \n",
      " |  tree_copy(self, id_objects={})\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from cvxpy.utilities.canonical.Canonical:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  canonical_form\n",
      " |      A lazily evaluated propery.\n",
      " |  \n",
      " |  expr\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(cvxpy.Problem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'cvxpy' has no attribute 'Bool'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-f0df21bc1f87>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# The variable we are solving for\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mselection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcvxpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# The sum of the weights should be less than or equal to P\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'cvxpy' has no attribute 'Bool'"
     ]
    }
   ],
   "source": [
    "import cvxpy\n",
    "import numpy as np\n",
    "\n",
    "# The data for the Knapsack problem\n",
    "# P is total weight capacity of sack\n",
    "# weights and utilities are also specified\n",
    "P = 165\n",
    "weights = np.array([23, 31, 29, 44, 53, 38, 63, 85, 89, 82])\n",
    "utilities = np.array([92, 57, 49, 68, 60, 43, 67, 84, 87, 72])\n",
    "\n",
    "# The variable we are solving for\n",
    "selection = cvxpy.Bool(len(weights))\n",
    "\n",
    "# The sum of the weights should be less than or equal to P\n",
    "weight_constraint = weights * selection <= P\n",
    "\n",
    "# Our total utility is the sum of the item utilities\n",
    "total_utility = utilities * selection\n",
    "\n",
    "# We tell cvxpy that we want to maximize total utility \n",
    "# subject to weight_constraint. All constraints in \n",
    "# cvxpy must be passed as a list\n",
    "knapsack_problem = cvxpy.Problem(cvxpy.Maximize(total_utility), [weight_constraint])\n",
    "\n",
    "# Solving the problem\n",
    "knapsack_problem.solve(solver=cvxpy.GLPK_MI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
